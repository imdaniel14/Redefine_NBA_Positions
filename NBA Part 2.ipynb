{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21e05e9",
   "metadata": {},
   "source": [
    "This is the second part of project. Please refer to part 1 for information on how the dataset was collected and cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - causes longer run times\n",
    "# The data frame that will be used has 120 columns to start with and I want to be able to see all of them\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Also, to see all rows\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1702f1d",
   "metadata": {},
   "source": [
    "# Initial Feature Selection & Data Transformation\n",
    "\n",
    "As shown below, this current dataset has 1,162 observations and 115 (numerical) features currently, so there is a further need to preprocess the data further. Here is what the following process will look like:\n",
    "\n",
    "- For some initial feature selection, I will employ a basic filter method: eliminate all redundant features as defined by having a Pearson's Correlation Coefficient value of over 0.98 (absolute value). This value is set semi-arbitrarily high because the purpose is to eliminate features that are completely explained (correlation coefficient = 1/-1) but some of the rate statistics have been rounded, so I'm using the value 0.98.\n",
    "\n",
    "- Next, an examination of the distributions of features will be done to determine whether there should be any transformations done to highly-skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af058718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('NBA_FINAL_DATA.csv', index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801222d",
   "metadata": {},
   "source": [
    "The data frame has 1162 observations and 120 features. The primary concern here is the curse of dimensionality; this is going to be address later with dimensionality reduction. However, before moving on to that step, I want to look at and study the dataset first. Before moving forward, I want to instantiate the features variable, X, with only numerical columns (also without age, games played)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a10ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(['PLAYER', 'TEAM', 'AGE', 'GP', 'POSITION'], axis=1)\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551189a",
   "metadata": {},
   "source": [
    "Having 115 features is definitely going to cause any sort of correlation heatmap to be an inefficient way of looking at heavily correlated features. However, I will create one just for visualization purposes which will give us a visual, high-level overview of what the correlation coefficients between the features look like where the absolute value of the correlation coefficients are greater than 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53b9e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60,40))\n",
    "mask = np.triu(np.ones_like(X.corr()))\n",
    "sns.heatmap(X.corr()[abs(X.corr())>=.98], mask=mask, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of All Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894c11a",
   "metadata": {},
   "source": [
    "From the visual above, there doesn't seem to be an overly large amount of redundant features (as defined by correlation > 0.98 mentioned above). I will take a look at all the columns and then select features to drop; of the pair of features who shared 0.98 correlation, I will personally decide which feature to drop. This decision will be made based off interpretability; for example (as seen below), between >=24FT_FGA and 3PA, I will drop the former because the latter name is more interpretable and used more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c5ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "\n",
    "corr.shape\n",
    "\n",
    "corr.abs().unstack().sort_values(ascending=False).drop_duplicates().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features \n",
    "X = X.drop(['%FGA_2PT', 'FGM_%AST', '2FGM_%AST', '%LOOSE_RECOVERED_OFF', '>=24FT_FGA', '>=24FT_FGM', 'SCREEN_AST_PTS', \n",
    "            '%TEAM_DREB', '%TEAM_REB', 'FGM', 'FTM', '3PM', '%TEAM_AST', '<8FT_FGM', '8-16_FGM'], axis=1)\n",
    "\n",
    "# Check number of features left\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130075c8",
   "metadata": {},
   "source": [
    "Next, I will check the distributions of the features and determine whether transformation should be applied. To determine whether a feature is heavily skewed, I will utilize a threshold: if abs(mean) >= abs(1.25 * median), then heavily skewed to the right and if abs(mean) <= abs(0.75 * median), then heavily skewed to the left. Before I check, I want to visualize the distributions for all features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function code modified from one provided by BrainStation\n",
    "def hist_create(df,column,transform = lambda x: x):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(transform(df[column]))\n",
    "    plt.title(f\"histogram of {column}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049dbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in X.columns:\n",
    "    \n",
    "    hist_create(X,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15689538",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_skewed = [column for column in X.columns if X[column].mean() >= 1.25*X[column].median()]\n",
    "\n",
    "print(f'Number of Right Skewed Features: {len(right_skewed)}')\n",
    "\n",
    "print(f'Right Skewed Columns: {right_skewed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_right_skewed = [column for column in X.columns if column not in right_skewed]\n",
    "print(f'Number of non-right skewed features: {len(not_right_skewed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f36184",
   "metadata": {},
   "source": [
    "For right-skewed data, I will consider applying log transformation. Before applying any actual transformation, using the create_hist function from earlier, visualize what a log transformation will look like for the features that are heavily right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b835f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in right_skewed:\n",
    "    hist_create(X,column,lambda x: np.log(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation\n",
    "for column in right_skewed:\n",
    "    X[column] = np.log(X[column] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_skewed = [column for column in not_right_skewed if abs(X[column].mean()) <= abs((.75*X[column].median()))]\n",
    "len(left_skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b14eab",
   "metadata": {},
   "source": [
    "# Validating Clustering Tendencies of Dataset\n",
    "\n",
    "Before moving on to the steps of dimensionality reduction and actual modelling, I wanted to perform a few quick checks to ensure that the dataset has clustering tendencies. These checks will be done once optimal number of clusters and other parameters are optimized as well but will also be done now without improving our dataset to get a baseline snapshot of our current dataset. There will be two methods to determine clustering tendencies:\n",
    "\n",
    "1) The first will be using visual heuristics - I will use PCA and TSNE to reduce the dimensionality of the dataset to 2 components purely for visualization purposes and perform a visual check to see if there are patterns that exhibit clustering tendencies.\n",
    "\n",
    "2) Afterwards, using the pyclustertend library, I will calculate the hopkins score - a metric that explains spatial randomness of the data. The pyclustertend library documentation as well as other online sources indicate that a score around 0.5 expresses non-clustering tendencies (data is uniformly distributed) and scores that tend to 0 indicate high clustering tendencies. Using the Pyclustertend library as well as other online sources, the threshold set will be 3.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bfaaab",
   "metadata": {},
   "source": [
    "### Visualizing in 2D with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "mm_scaler = MinMaxScaler()\n",
    "s_scaler = StandardScaler()\n",
    "r_scaler = RobustScaler()\n",
    "\n",
    "X_mm = mm_scaler.fit_transform(X)\n",
    "X_ss = s_scaler.fit_transform(X)\n",
    "X_rs = r_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit_transform pca and print explained variance ratio sum\n",
    "X_mm_pca = pca.fit_transform(X_mm)\n",
    "print(f'Explained Variance Ratio for X_mm: {pca.explained_variance_ratio_.sum()}')\n",
    "\n",
    "X_ss_pca = pca.fit_transform(X_ss)\n",
    "print(f'Explained Variance Ratio for X_ss: {pca.explained_variance_ratio_.sum()}')\n",
    "\n",
    "X_rs_pca = pca.fit_transform(X_rs)\n",
    "print(f'Explained Variance Ratio for X_rs: {pca.explained_variance_ratio_.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c41bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(12,20))\n",
    "fig.suptitle('PCA: 2 Dimensional Visualization of NBA Dataset')\n",
    "ax1.scatter(X_mm_pca[:,0], X_mm_pca[:,1])\n",
    "ax1.set_title('MinMax Scaled')\n",
    "\n",
    "ax2.scatter(X_ss_pca[:,0], X_ss_pca[:,1])\n",
    "ax2.set_title('Standard Scaled')\n",
    "\n",
    "ax3.scatter(X_rs_pca[:,0], X_rs_pca[:,1])\n",
    "ax3.set_title('Robust Scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b52700",
   "metadata": {},
   "source": [
    "### Visualizing in 2D with TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=14)\n",
    "\n",
    "X_mm_tsne = tsne.fit_transform(X_mm)\n",
    "X_ss_tsne = tsne.fit_transform(X_ss)\n",
    "X_rs_tsne = tsne.fit_transform(X_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c00f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(12,20))\n",
    "fig.suptitle('TSNE: 2 Dimensional Visualization of NBA Dataset')\n",
    "ax1.scatter(X_mm_tsne[:,0], X_mm_tsne[:,1])\n",
    "ax1.set_title('MinMax Scaled')\n",
    "\n",
    "ax2.scatter(X_ss_tsne[:,0], X_ss_tsne[:,1])\n",
    "ax2.set_title('Standard Scaled')\n",
    "\n",
    "ax3.scatter(X_rs_tsne[:,0], X_rs_tsne[:,1])\n",
    "ax3.set_title('Robust Scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d06093",
   "metadata": {},
   "source": [
    "From the visuals above, the clustering tendencies of the datasets do appear a bit low. However, that being said, for the TSNE visuals, the minmax-scaled data and the robust-scaled data clearly exhibit at least 2-3 distinct clusters and one could make the argument that the standard-scaled data shows 3 distinct clusters.\n",
    "\n",
    "The next check will be with the hopkins test as mentioned earlier using a threshold of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustertend import hopkins\n",
    "\n",
    "print(f'Hopkins Test Score For X_mm: {hopkins(X_mm, X_mm.shape[0])}')\n",
    "print(f'Hopkins Test Score For X_ss: {hopkins(X_ss, X_ss.shape[0])}')\n",
    "print(f'Hopkins Test Score For X_rs: {hopkins(X_rs, X_rs.shape[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5a3c4",
   "metadata": {},
   "source": [
    "The hopkins test scores for all three differently-scaled datasets - using the threshold of 0.3 - indicates that the dataset does exhibit clustering tendencies (the test scores are all relatively similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00da713",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Even after I manually removed redundant features earlier (using threshold of 0.98 correlation coefficient), the dataset contains 100 features. As a result, the curse of dimensionality is a big concern that needs to be addressed before clustering. \n",
    "\n",
    "Ideally, I would want to apply feature selection first in order to narrow the amount of features to the most important ones. That being said, feature selection for unsupervised learning is very difficult to do and can inject a lot of bias into the dataset. I have split this notebook/project into two different sections:\n",
    "\n",
    "1) The first section (the one that will be performed now) is a more conservative process. I will undergo the regular clustering processes by scaling the dataset, applying dimensionality reduction only, and then clustering.\n",
    "\n",
    "2) For the second section (after first section is complete), I will again perform scaling on the dataset, applying feature selection (through PFA), then applying dimensionality reduction, and then clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03eb328",
   "metadata": {},
   "source": [
    "# PCA\n",
    "There are two different dimensionality reduction processes that I will perform. The first is PCA and in order to choose the optimal number of PCs, I will use an elbow plot and check the optimal number of PCs for the three, differently-scaled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac269a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Minmax scaled data\n",
    "pca = PCA()\n",
    "\n",
    "pca_mm = pca.fit(X_mm)\n",
    "\n",
    "# Instantiate variable for explained variance ratio (minmax scaled dataset)\n",
    "expl_var_mm = pca_mm.explained_variance_ratio_\n",
    "\n",
    "# Plot scree (eblow) plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1,101),expl_var_mm,marker='.')\n",
    "plt.title('Proportion of Variance Explained for Number of PCs')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xticks(range(1,101,2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab090252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much variance can be explained with 5 components\n",
    "np.cumsum(expl_var_mm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1ed63",
   "metadata": {},
   "source": [
    "With the elbow plot, the optimal number of components was shown to be 5 components that explained about 70% of the total variance in the dataset. I also want to quickly evaluate how many additional components needed to increase this explained variance ratio significantly.\n",
    "\n",
    "Arbitrarily selecting 10% as a measure of significant increase in explained variance, I will plot of cumulative sum graph to see how many additional components would be needed to get an explained variance ratio of 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f781f12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot out the cumulative sum graph\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(1,101), np.cumsum(expl_var_mm), marker='.')\n",
    "plt.axhline(0.8, c='r', linestyle='--')\n",
    "plt.title('Cumulative Sum of Explained Variance Against Number of PCs')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Cumulative Sum of Explained Variance')\n",
    "plt.xticks(range(1,101,2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd5e57",
   "metadata": {},
   "source": [
    "The graph indicates that 10 more components would be needed (100% increase) in order to increase the explained variance by 10%. I will choose to stick with n_components=5 which provides enough explained variance while maintaining good interpretability. I will continue these steps for the standard-scaled and robust-scaled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20073d63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard scaled data\n",
    "pca = PCA()\n",
    "pca_ss = pca.fit(X_ss)\n",
    "\n",
    "expl_var_ss = pca_ss.explained_variance_ratio_\n",
    "\n",
    "# Plot scree (eblow) plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1,101),expl_var_ss,marker='.')\n",
    "plt.title('Proportion of Variance Explained for Number of PCs')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xticks(range(1,101,2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Explained Variance for 5 Components: {np.cumsum(expl_var_ss[:5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dea1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust-scaled data\n",
    "pca = PCA()\n",
    "pca_rs = pca.fit(X_rs)\n",
    "\n",
    "expl_var_rs = pca_rs.explained_variance_ratio_\n",
    "\n",
    "# Plot scree (eblow) plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1,101),expl_var_rs,marker='.')\n",
    "plt.title('Proportion of Variance Explained for Number of PCs')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xticks(range(1,101,2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Explained Variance for 8 Components: {np.cumsum(expl_var_rs[:8])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e7c14",
   "metadata": {},
   "source": [
    "For the standard scaled dataset, I select 5 components (explained variance of 0.65) and for the robust scaled dataset, I selected 8 components (explained variance of 0.7).\n",
    "\n",
    "Summary:\n",
    "- minmax data: 5 components\n",
    "- standard data: 5 components\n",
    "- robust data: 8 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all datasets\n",
    "X_mm_pca = pca_mm.transform(X_mm)\n",
    "X_ss_pca = pca_ss.transform(X_ss)\n",
    "X_rs_pca = pca_rs.transform(X_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a75c6",
   "metadata": {},
   "source": [
    "The next step will be to perform the actual clustering with these finalized datasets. I will evaluate the clustering performance using inertia and silhouette scores. These are the two most common metrics implemented in the sci-kit library used for unsupervised learning when ground truth labels are unknown. For KMeans hyperparameters, I am semi-arbitrarily selecting init='k-means++' to ensure centers are are initialized with some distance apart, and increasing n_init and max_iter to find a more stable solution (balancing with computational costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4d1b0",
   "metadata": {},
   "source": [
    "### MinMax Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6776b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Get inertia values as a function of different cluster sizes\n",
    "ks = np.arange(1,15)\n",
    "\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_mm_pca)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot inertias against k values (number of clusters)    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('inertia')\n",
    "plt.title('K Means: Number of Clusters (k) vs Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa79f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "ks = np.arange(2,15)\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_mm_pca)\n",
    "    \n",
    "    sil_scores.append(silhouette_score(X_mm_pca, kmeans.labels_))\n",
    "\n",
    "# Plot sil_scores against number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, sil_scores, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('sil_scores')\n",
    "plt.title('K Means: Number of Clusters (k) vs Silhouette Scores')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/minmax_silscore.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58457be",
   "metadata": {},
   "source": [
    "For the minmax dataset, both metrics indicate that the optimal number of clusters is 3 the a silhouette score of about 0.21. I will perform the same steps for the other datasets before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007dd36",
   "metadata": {},
   "source": [
    "### Standard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get inertia values as a function of different cluster sizes\n",
    "ks = np.arange(1,15)\n",
    "\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_ss_pca)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot inertias against k values (number of clusters)    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('inertia')\n",
    "plt.title('K Means: Number of Clusters (k) vs Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(2,15)\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_ss_pca)\n",
    "    \n",
    "    sil_scores.append(silhouette_score(X_ss_pca, kmeans.labels_))\n",
    "\n",
    "# Plot sil_scores against number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, sil_scores, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('sil_scores')\n",
    "plt.title('K Means: Number of Clusters (k) vs Silhouette Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c80c7",
   "metadata": {},
   "source": [
    "### Robust Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1,15)\n",
    "\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_rs_pca)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot inertias against k values (number of clusters)    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('inertia')\n",
    "plt.title('K Means: Number of Clusters (k) vs Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(2,15)\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_rs_pca)\n",
    "    \n",
    "    sil_scores.append(silhouette_score(X_rs_pca, kmeans.labels_))\n",
    "\n",
    "# Plot sil_scores against number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, sil_scores, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('sil_scores')\n",
    "plt.title('K Means: Number of Clusters (k) vs Silhouette Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5b476",
   "metadata": {},
   "source": [
    "From the visuals above, the optimal number of clusters for all datasets was 3. However, the minmax dataset provided the highest silhouette score with 0.21. This silhouette score is quite low and indicates that I should try to increase this score before moving forward. There are two steps to increase the silhouette score for the KMeans model:\n",
    "\n",
    "1) I can play around with the number of components for PCA-transformed data and add additional components to achieve a higher explained variance ratio.\n",
    "\n",
    "2) I can try a different type of dimensionality reduction.\n",
    "\n",
    "I will proceed with the latter option (reasoning is the same as to why I didn't increase the explained variance ratio earlier). For now, I will implement dimensionality reduction using Linear Discriminant Analysis. This makes sense because LDA uses class labels to find a subspace that maximizes class seperability. I will apply LDA with the classes before the NBA players' traditional positions. For LDA, scaling has no effect, so I will only apply LDA on the minmax data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539a11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Instatiate target (clases) as the traditional positions\n",
    "y = df['POSITION']\n",
    "\n",
    "lda = LDA()\n",
    "lda_mm = lda.fit(X_mm, y)\n",
    "\n",
    "\n",
    "# Instantiate variable for explained variance ratio (minmax scaled dataset)\n",
    "expl_var_mm = lda_mm.explained_variance_ratio_\n",
    "\n",
    "print(expl_var_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34615812",
   "metadata": {},
   "source": [
    "LDA captured about 95% of the total variance in just two components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7610b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use 2 components\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X_mm, y)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X_lda[:,0], X_lda[:,1])\n",
    "plt.title('LDA: 2 Dimensional Visualization of NBA Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1,15)\n",
    "\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_lda)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot inertias against k values (number of clusters)    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('inertia')\n",
    "plt.title('K Means: Number of Clusters (k) vs Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed55c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(2,15)\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=10, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_lda)\n",
    "    \n",
    "    sil_scores.append(silhouette_score(X_lda, kmeans.labels_))\n",
    "\n",
    "# Plot sil_scores against number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, sil_scores, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('sil_scores')\n",
    "plt.title('K Means: Number of Clusters (k) vs Silhouette Scores')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/lda_silscore.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f2a20",
   "metadata": {},
   "source": [
    "The silhouette score here is significantly better than our PCA-transformed dataset. Additionally, the visualization definitely showcases higher clustering tendencies than with our PCA/TSNE visualizations. However, there is a big issue here indicated that LDA is able to capture 95% of the total variance with just the first 2 components. LDA is well-known to be prone to overfitting and that is what may be happening here. To address this issue, I will first play around with reducing the number of features before even applying LDA dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cd1ff",
   "metadata": {},
   "source": [
    "# Additional Feature Selection & Dimensionality Reduction\n",
    "\n",
    "I will be performing additional filter methods for feature selection to reduce the number of features:\n",
    "\n",
    "1) Variance threshold to eliminate constant columns (might be 0 but will check)\n",
    "\n",
    "2) Using Pearson's Correlation Coefficient and eliminating features that share a coefficient value of over 92% and I will be manually dropping these to keep the more interpretable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize before applying variance threshold\n",
    "\n",
    "X_mm = mm_scaler.fit_transform(X)\n",
    "\n",
    "X_mm = pd.DataFrame(data=X_mm, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a704cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "practice = selector.fit_transform(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "practice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999bdce",
   "metadata": {},
   "source": [
    "No columns were dropped by eliminating constant features - moving on to filtering out using correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842f7e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "\n",
    "corr.shape\n",
    "\n",
    "corr.abs().unstack().sort_values(ascending=False).drop_duplicates().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ada781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X.drop(['%TEAM_OREB', 'FGA', '%TEAM_3PA', '%FGA_3PT', '<8FT_FGA', '%TEAM_FTA', '%TEAM_FGA', 'DREB', 'FTA', \n",
    "            '16-24_FGA', '%TEAM_FGM', '%FGA_3PT', '%PTS_3PT', 'DREB%', '%TEAM_FTA', 'eFG%', 'CONTEST_SHOTS', \n",
    "            'OPP_PAINT_PTS', '%PTS_2PT', 'DEFLECTIONS', '%TEAM_FTM', '>=24FT_FG%'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many features remaining\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38621543",
   "metadata": {},
   "source": [
    "We have 80 features after removing the heavily correlated features. This is still a large number of features but I will apply dimensionality reduction moving forward. From previously, it's clear that applying LDA (with the traditional 5 positions being the labels) provided a much higher silhouette score. However, the problem was that there seemed to be overfitting because the first 2 components accounts for about 95% of the variation. \n",
    "\n",
    "I will be moving forward with the LDA dataset; however, I need to ensure that the explained variance isn't so high because it may adversely affect clustering algorithms. Additionally, from this step forward, I will only be using minmax-scaled data as minmax-scaler seems to be the most common scaler for clustering and minmax makes a lot of sense when comparing statistics for NBA players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555769c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_mm = mm_scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the same method as previously with new dataset containing 80 features instead of 100\n",
    "lda = LDA()\n",
    "X_lda = lda.fit(X_mm, y)\n",
    "\n",
    "expl_var_lda = X_lda.explained_variance_ratio_\n",
    "\n",
    "print(expl_var_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b21a0a",
   "metadata": {},
   "source": [
    "It's clear that even with the new dataset, there appears to be overfitting with an explained variance ratio of over 0.95 with just the first two components. In order to ensure that overfitting isn't a problem moving forward, I will be using LDA with the 'eigen' solver that allows a shrinkage parameter. The optimal shrinkage parameter will be determined somewhat arbitrarily; I will select the parameter that provides around 0.8-0.85 explained variance in the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrink_vals = [0.009, 0.008, 0.007, 0.006, 0.005, 0.004, 0.003, 0.002, 0.001]\n",
    "\n",
    "for val in shrink_vals:\n",
    "    lda = LDA(solver='eigen', shrinkage=val, n_components=2)\n",
    "    X_lda = lda.fit(X_mm, y)\n",
    "    \n",
    "    explained_var = X_lda.explained_variance_ratio_\n",
    "    \n",
    "    if (explained_var[0] + explained_var[1]) >= 0.7:\n",
    "        print(f'Shrinkage: {val}', f'Component 1: {explained_var[0]}', f'Component 2: {explained_var[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c8467",
   "metadata": {},
   "source": [
    "I will be selecting the shrinkage value to be 0.002 which provides about 84% explained variance in the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3337e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction to dataset\n",
    "lda = LDA(solver='eigen', shrinkage=0.002, n_components=2)\n",
    "\n",
    "X_lda = lda.fit_transform(X_mm, y)\n",
    "\n",
    "print(X_lda.shape)\n",
    "print(lda.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_lda[:,0], X_lda[:,1], c='pink')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('LDA: 2 Dimensional Visualization of NBA Dataset')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/lda_featurespace.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691a6c0",
   "metadata": {},
   "source": [
    "# Clustering Models\n",
    "\n",
    "I will start to perform some clustering and analysis of the clusters. The optimal number of clusters will be selected by examining inertia, silhouette scores, and davies-bouldin scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0d3f3",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Inertias\n",
    "ks = np.arange(1,15)\n",
    "\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_lda)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot inertias against k values (number of clusters)    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.xlim(1, 14.5)\n",
    "plt.axvline(x=3, c='red')\n",
    "plt.ylabel('inertia')\n",
    "plt.title('K Means: Number of Clusters (k) vs Inertia')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/kmeans_intertia.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fa553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Silhouette Scores\n",
    "ks = np.arange(2,15)\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_lda)\n",
    "    \n",
    "    sil_scores.append(silhouette_score(X_lda, kmeans.labels_))\n",
    "\n",
    "# Plot sil_scores against number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, sil_scores, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('sil_scores')\n",
    "plt.title('K Means: Number of Clusters (k) vs Silhouette Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2352c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "ks = np.arange(2,15)\n",
    "\n",
    "db_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "    kmeans.fit(X_lda)\n",
    "    \n",
    "    db_scores.append(davies_bouldin_score(X_lda, kmeans.labels_))\n",
    "\n",
    "# Plot sil_scores against number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ks, db_scores, marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel('K values')\n",
    "plt.ylabel('DB_scores')\n",
    "plt.title('K Means: Number of Clusters (k) vs Davies Bouldin Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a9b7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks = np.arange(2,15)\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ks, db_scores, marker='o', label='DBI score')\n",
    "plt.plot(ks, sil_scores, marker='o', label='silhouette score')\n",
    "plt.grid()\n",
    "plt.xlabel('K: Number of Clusters')\n",
    "plt.ylabel('Performance Metrics')\n",
    "plt.xlim(1, 14.5)\n",
    "plt.title('K Means: Number of Clusters (k) vs Metrics')\n",
    "plt.axvline(x=3, c='red')\n",
    "plt.legend()\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/kmeans_metrics1.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e147f3",
   "metadata": {},
   "source": [
    "Using all the metrics above, I will be selecting 3 clusters as the optimal number of clusters to examine initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63454fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster labels\n",
    "kmeans = KMeans(n_clusters=3, random_state=12, init='k-means++', n_init=50, max_iter=500)\n",
    "kmeans.fit(X_lda)\n",
    "kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c75a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('dark_background')\n",
    "plt.scatter(X_lda[:,0], X_lda[:,1], c=kmeans_labels, cmap='gist_rainbow')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('LDA: NBA Players Clusters 2020-2022')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/3clusters1.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5754964",
   "metadata": {},
   "source": [
    "This definitely does look like the most naturally separated areas visually. I will analyze the information further by checking the variance within the statistics for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79226d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = mm_scaler.fit_transform(X)\n",
    "X_mm = pd.DataFrame(data=X_mm, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f03510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans3_df = X_mm.copy()\n",
    "\n",
    "# Create labels column\n",
    "kmeans3_df['kmeans_labels'] = kmeans_labels\n",
    "\n",
    "# Check clusters' statistical means\n",
    "grouped_kmeans3 = kmeans3_df.groupby('kmeans_labels', as_index=False).mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9108d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_kmeans3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e47d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_kmeans3.columns = ['Cluster 0', 'Cluster 1', 'Cluster 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067d32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_kmeans3['variance'] = grouped_kmeans3.var(axis=1)\n",
    "\n",
    "grouped_kmeans3.sort_values(by='variance', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding features with highest variance between clusters\n",
    "grouped_kmeans3_sorted = grouped_kmeans3.sort_values(by='variance', ascending=False).head(20)\n",
    "\n",
    "# creating separate table\n",
    "grouped_kmeans3_sorted = grouped_kmeans3_sorted.drop('kmeans_labels', axis=0)\n",
    "\n",
    "# Check\n",
    "grouped_kmeans3_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39876549",
   "metadata": {},
   "source": [
    "In order to get a better sense of the variability of features between the clusters, a graph will be created to show the features with highest variance between clusters. However, it is important to keep in mind that this isn't a way of measuring 'feature importance' - in other words, which features were the most important to determining clusters that players belonged to. Feature importance will be explored later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bee1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set plot style: grey grid in the background:\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x=grouped_kmeans3_sorted['variance'], y=grouped_kmeans3_sorted.index)\n",
    "plt.title('Top 20 Features by Between-Cluster Variance')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Features')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/report_01.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97e1e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans3 = df.copy()\n",
    "df_kmeans3['kmeans_label'] = kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f420f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans3.loc[df_kmeans3['kmeans_label'] == 0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d753d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans3.loc[df_kmeans3['kmeans_label'] == 1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e528eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans3.loc[df_kmeans3['kmeans_label'] == 2].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0de13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans3.loc[df_kmeans3['kmeans_label'] == 0, 'kmeans_label'] = 'Cluster 0'\n",
    "df_kmeans3.loc[df_kmeans3['kmeans_label'] == 1, 'kmeans_label'] = 'Cluster 1'\n",
    "df_kmeans3.loc[df_kmeans3['kmeans_label'] == 2, 'kmeans_label'] = 'Cluster 2'\n",
    "\n",
    "# For visuals\n",
    "df_kmeans3.to_csv('C:/Users/imdan/downloads/data/df_kmeans3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18633bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans3[['POSITION', 'kmeans_label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20297ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans3_ = df_kmeans3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca029fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans3_.loc[df_kmeans3['POSITION'] == 'PG', 'POSITION'] = 'Guard'\n",
    "df_kmeans3_.loc[df_kmeans3['POSITION'] == 'SG', 'POSITION'] = 'Guard'\n",
    "df_kmeans3_.loc[df_kmeans3['POSITION'] == 'SF', 'POSITION'] = 'Forward'\n",
    "df_kmeans3_.loc[df_kmeans3['POSITION'] == 'PF', 'POSITION'] = 'Forward'\n",
    "df_kmeans3_.loc[df_kmeans3['POSITION'] == 'C', 'POSITION'] = 'Center'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ed578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot counts of clusters labels by positions\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.style.use('seaborn-pastel')\n",
    "plt.title('Number of Players per Cluster (3)')\n",
    "plt.xlabel('NBA Position')\n",
    "plt.ylabel('Number of Players')\n",
    "sns.countplot(data=df_kmeans3_, x='POSITION', hue='kmeans_label').set(xlabel='NBA Position', ylabel='Number of Players')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/3clusters.jpeg', facecolor='lightcyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot counts of clusters labels by positions\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Counts of Cluster Labels by Traditional Positions')\n",
    "sns.countplot(data=df_kmeans3, x='POSITION', hue='kmeans_label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2e1c3",
   "metadata": {},
   "source": [
    "From above, it's clear that for the most part, the clusters are separated similarly to how the traditional positions were separated (mostly by size). Cluster 0 indicated players who are smaller and play like guards (PGs and SGs); Cluster 1 reflects mid-sized wing players (SGs, SFs, PFs) and Cluster 2 highlights bigs (PFs and Cs). The separation is quite clear no Centers (C) in cluster 0.\n",
    "\n",
    "This was to be expected and not surprising at all. Intuitively, it's clear that players of different sizes are going to show the clearest separation because a player who is a certain size cannot do things like players of a different size.\n",
    "\n",
    "The next step is where the real analysis begins. Similar to other unsupervised ML projects, because there are no ground truth labels, there are also no true measures of the optimal numbers of clusters. For this project, I will refer back to my business problem and the value that this project will add: identify/sub-segment players into roles that are more clearly defined than the current 5 traditional positions. Consequently, I believe that to add more value, a larger number of clusters is going to be more valuable. Referring back to the graph of silhouette scores earlier, I will proceed with analyzing 7 separate clusters. The reason is that 7 provides me with a high number of segments while also keeping the silhouette score at around 0.38 which is generally considered to indicate good separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 clusters\n",
    "kmeans = KMeans(n_clusters=7, random_state=1, init='k-means++', n_init=100, max_iter=500)\n",
    "kmeans.fit(X_lda)\n",
    "kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7500a11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.style.use('dark_background')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.scatter(X_lda[:,0], X_lda[:,1], c=kmeans_labels, cmap='rainbow')\n",
    "plt.title('Final KMeans: NBA Players Clusters 2020-2022')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/7clusters0.jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans7_df = X_mm.copy()\n",
    "\n",
    "# Create labels column\n",
    "kmeans7_df['kmeans_labels'] = kmeans_labels\n",
    "\n",
    "# Check clusters' statistical means\n",
    "grouped_kmeans7 = kmeans7_df.groupby('kmeans_labels', as_index=False).mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759eac36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_kmeans7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2759ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_kmeans7.columns = ['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', \n",
    "                           'Cluster 6']\n",
    "\n",
    "grouped_kmeans7['variance'] = grouped_kmeans7.var(axis=1)\n",
    "\n",
    "grouped_kmeans7_sorted = grouped_kmeans7.drop('kmeans_labels', axis=0).sort_values(by='variance', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot style: grey grid in the background:\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=grouped_kmeans7_sorted['variance'], y=grouped_kmeans7_sorted.index)\n",
    "plt.title('Top 20 Features by Between-Cluster Variance: 7 Clusters')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Features')\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/report_1.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rankings\n",
    "ranked_kmeans7 = kmeans7_df.groupby('kmeans_labels', as_index=False).mean().rank(method='max').T\n",
    "\n",
    "ranked_kmeans7.columns = ['Cluster 0' , 'Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6']\n",
    "\n",
    "ranked_kmeans7 = ranked_kmeans7.drop('kmeans_labels', axis=0)\n",
    "\n",
    "ranked_kmeans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe616e9",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Before moving on to analyzing the individual clusters separately, I want to explore which features are the most important when determining these clusters. A big shortcoming of the KMeans algorithm is that all the feature dimensions are weighed equally. In order to get an idea of which features are more relevant when determining the 7 found clusters, I will be training a random forest model with the cluster labels as the target variable. \n",
    "\n",
    "This random forest model - provided that a decent accuracy score is achieved - can tell us which features were the most important or 'predictive.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "# Initialize estimator - random forest classifier\n",
    "estimators = [('normalise', MinMaxScaler()),\n",
    "              ('rfc', RandomForestClassifier())]\n",
    "\n",
    "# Initialize pipeline\n",
    "pipe = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a105b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial gridsearchCV to test for best model hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "params = {'normalise': [MinMaxScaler()],\n",
    "          'rfc__n_estimators': [25, 50, 75, 100, 125, 150, 175, 200],\n",
    "          'rfc__max_depth': [3, 5, 7, 9], \n",
    "          'rfc__min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "          'rfc__max_features': ['sqrt', 'log2', None, 0.2]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=12)\n",
    "    \n",
    "fitted_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameter values\n",
    "fitted_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd115895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "fitted_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch 2\n",
    "estimators = [('normalise', MinMaxScaler()),\n",
    "              ('rfc', RandomForestClassifier(max_depth=9))]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "params = {'normalise': [MinMaxScaler()],\n",
    "          'rfc__n_estimators': [200, 250, 300, 350, 400, 450, 500],\n",
    "          'rfc__max_features': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=12)\n",
    "    \n",
    "fitted_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c46700",
   "metadata": {},
   "source": [
    "With a best accuracy score of around 76% achieved - I will train the random forest model now. I will also be adding min_samples_leaf=10 semi-arbitrarily in order to limit overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1add1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.33, random_state=12)\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "mm_scaler.fit_transform(X_train)\n",
    "mm_scaler.transform(X_validation)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_depth=9, max_features=0.2, min_samples_leaf=10)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Train score: {rfc.score(X_train, y_train)}\")\n",
    "print(f\"Validation score: {rfc.score(X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Code retrieved and modified from sklearn documentation\n",
    "result = permutation_importance(\n",
    "    rfc, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "\n",
    "importance = importances.T\n",
    "importance[10] = np.mean(importance, axis=1)\n",
    "importance_t10 = importance.sort_values(by=10, ascending=False).head(10).T\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(15, 6))\n",
    "ax = importance_t10.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "plt.savefig(fname='C:/Users/imdan/downloads/data/feature_importance1.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aecc79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importance_b10 = importance.sort_values(by=10).head(10).T\n",
    "\n",
    "ax = importance_b10.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6636f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if any features had 0 impact (training set)\n",
    "importance.loc[importance[10] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773272d4",
   "metadata": {},
   "source": [
    "I also want to check feature importance for the validation set as well - keeping in mind that the accuracy score was about 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    rfc, X_validation, y_validation, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "\n",
    "importance = importances.T\n",
    "importance[10] = np.mean(importance, axis=1)\n",
    "importance_t10 = importance.sort_values(by=10, ascending=False).head(10).T\n",
    "\n",
    "ax = importance_t10.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd379d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_b10 = importance.sort_values(by=10).head(10).T\n",
    "\n",
    "ax = importance_b10.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if any features had 0 impact (training set)\n",
    "importance.loc[importance[10] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054aa5b",
   "metadata": {},
   "source": [
    "Looking mostly at the training set, it was interesting that height wasn't the most predictive as screen_assist was just higher. It is a bit difficult to interpret why that is but it may be that since there are multiple clusters/groups of \"big\" players now and one way that they are being differentiated is through screen assists.\n",
    "\n",
    "More interesting features that were also importance were the following:\n",
    "- 2FGM_%UAST (% of 2pt field goals that were unassisted by another player)\n",
    "- %PTS_FBREAK (% of points that were from fast breaks)\n",
    "\n",
    "On the other hand, there were also some features that adversely affected accuracy scores such as OREB%, 3FGM_%AST, etc... \n",
    "\n",
    "Lastly, triple doubles did not affect model accuracy at all which is not surprising since they don't occur very frequently; when they do, likely random chance played a factor as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c925a",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd3855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine Cluster 0\n",
    "print(grouped_kmeans7['Cluster 0'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 0'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c013e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans7 = df.copy()\n",
    "df_kmeans7['kmeans_label'] = kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169a97a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 0].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bb5c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 0'] == 7.0, 'Cluster 0'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 0'] == 1.0, 'Cluster 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8adae",
   "metadata": {},
   "source": [
    "Cluster 0 key characteristics:\n",
    "- Excellent shot creation (2FGM_%UAST, 3FGM_%AUST, PULLUP_PTS, PULLUP_FG%))\n",
    "- High scoring (PTS, FT%)\n",
    "- 2nd best passing cluster (AST, AST%, AST/TO, AST_RATIO)\n",
    "- Inefficiency (FG%, TS%)\n",
    "- 3 level scoring (DRIVE_PTS, %PTS_MidRange, 16-24FGM, 8-16_FG%, 3P%)\n",
    "- High turnover (TOV)\n",
    "- Bad inside-defensive stats (BLK, REB, CONTEST_2PT)\n",
    "- High usage (USG%)\n",
    "- Small in size (Height, Weight)\n",
    "\n",
    "The label that I will use to describe cluster 0: Offensive-Minded guards.\n",
    "\n",
    "Notable examples: \n",
    "- Stephen Curry (2021)\n",
    "- Shai Gilgeous-Alexander (2020-2022)\n",
    "- C.J McCollum (2020-2022)\n",
    "- Jordan Poole (2020-2022)\n",
    "- Terry Rozier (2020-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011aeaef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine Cluster 1\n",
    "print(grouped_kmeans7['Cluster 1'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 1'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5f718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 1].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa91daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 1'] == 6.0, 'Cluster 1'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 1'] == 3.0, 'Cluster 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4875b",
   "metadata": {},
   "source": [
    "Cluster 1 Characteristics:\n",
    "- Efficiency (FG%, TS%, PIE, <8FT_FG%)\n",
    "- Second biggest group\n",
    "- Plays inside offensively (PAINTTOUCH_PTS, POSTTOUCH_PTS, ELBOWTOUCH_PTS, %PTS_PAINT, PAINT_PTS)\n",
    "- High CONTEST_3PT, PACE, AST% despite size\n",
    "\n",
    "\n",
    "Label 1 players: Mobile Bigs\n",
    "\n",
    "Notable Players:\n",
    "- Karl Anthony Towns (2021-2022)\n",
    "- Anthony Davis (2020-2022)\n",
    "- Kristaps Porzingis (2020-2022)\n",
    "- Christian Wood (2020-2022)\n",
    "- John Collins (2020-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Cluster 2\n",
    "print(grouped_kmeans7['Cluster 2'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 2'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71f7b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 2].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80960bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 2'] == 7.0, 'Cluster 2'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 2'] == 2.0, 'Cluster 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ad519",
   "metadata": {},
   "source": [
    "Cluster 2 Characteristics:\n",
    "- Fastest-paced players (PACE, FASTB_PTS, %PTS_FASTB, %PTS_TO)\n",
    "- Outside scoring (3PA, %TEAM_3PM, 3P%, 3FGM_%UAST, CATCHSHOOT_FG%, CATCHSHOOT_PTS)\n",
    "- 3 level scoring (16-24FGM, DRIVE_PTS)\n",
    "- Turnover-heavy (TO_RATIO, OPP_PTS_OFF_TOV)\n",
    "- Medium size\n",
    "- Lack of inside scoring (%PTS_PAINT, PAINT_PTS, 2ND_PTS, PF_DRAWN)\n",
    "\n",
    "Cluster 2 Players: Offensive-Minded Perimeter Wings\n",
    "Notable players: Devin Booker (2020-2022), Zach Lavine (2020-2022), Paul George (2020-2022), Bradley Beal (2020-2021), Anthony Edwards (2021-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Cluster 3\n",
    "print(grouped_kmeans7['Cluster 3'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 3'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a2813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 3].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d003ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 3'] == 5.0, 'Cluster 3'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 3'] == 3.0, 'Cluster 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052817a4",
   "metadata": {},
   "source": [
    "Cluster 3 Characteristics:\n",
    "- Great defensive stats (DEFRTG, CONTEST_3PT)\n",
    "- This group is a bit more unique where they don't rank particularly amazingly/terribly in many categories and instaed is more middle-of-the-pack for a large number of statistics.\n",
    "- wing/big size (3rd biggest group of players) -> will label as power forwards\n",
    "\n",
    "Cluster 3: Versatile Power Forwards\n",
    "\n",
    "Notable players\n",
    "- Giannis Antetokounmpo (2020-2022)\n",
    "- LeBron James (2022)\n",
    "- Pascal Siakam (2020-2022)\n",
    "- Julius Randle (2020-2022)\n",
    "- Carmelo Anthony (2020-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Cluster 4\n",
    "print(grouped_kmeans7['Cluster 4'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 4'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c03202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 4].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a8e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 4'] == 7.0, 'Cluster 4'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 4'] == 1.0, 'Cluster 4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d4448",
   "metadata": {},
   "source": [
    "Cluster 4 Characteristics:\n",
    "- Efficient (FG%, TS%)\n",
    "- Biggest Players\n",
    "- Worst shot-creation (unassisted fgs)\n",
    "- Best \"traditional bigs\" stats (BLK, REB, 2ND_PTS, %PTS_FT, %PTS_BLK)\n",
    "- Lack of outside scoring (3PA, 3P%, %PTS_MidRange)\n",
    "\n",
    "Cluster 4 players: Stationary Interior Bigs\n",
    "\n",
    "\n",
    "Notable Players:\n",
    "- Joel Embiid (2020-2021)\n",
    "- Nikola Jokic (2020-2022)\n",
    "- Domantas Sabonis (2020-2022)\n",
    "- Jonas Valanciunas (2020-2022)\n",
    "- Bam Adebayo (2020-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98c018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine Cluster 5\n",
    "print(grouped_kmeans7['Cluster 5'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 5'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9f060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 5].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f605993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 5'] == 6.0, 'Cluster 5'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 5'] == 2.0, 'Cluster 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76b198",
   "metadata": {},
   "source": [
    "Cluster 5 Characteristics:\n",
    "- Highest CATCHSHOOT_PTS (also 3FGM_%AST)\n",
    "- Fast-paced play (PACE, %PTS_FASTB, %PTS_TO)\n",
    "- Lowest usage group (USG%, TOV)\n",
    "- Low scoring (PTS, %TEAM_PTS)\n",
    "- Second highest CONTEST_3PT\n",
    "- Wing-sized\n",
    "\n",
    "Cluster 5: 3-D Wings\n",
    "\n",
    "Notable Players:\n",
    "- Jayson Tatum (2020-2022)\n",
    "- Kevin Durant (2022)\n",
    "- Jaylen Brown (2020-2021)\n",
    "- Jimmy Butler (2020, 2022)\n",
    "- Klay Thompson (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c28bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine Cluster 6\n",
    "print(grouped_kmeans7['Cluster 6'].sort_values(ascending=False).head(15))\n",
    "print()\n",
    "print(grouped_kmeans7['Cluster 6'].sort_values(ascending=True).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be85127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 6].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7994216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 6'] == 7.0, 'Cluster 6'])\n",
    "print()\n",
    "print(ranked_kmeans7.loc[ranked_kmeans7['Cluster 6'] == 2.0, 'Cluster 6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f32fd3",
   "metadata": {},
   "source": [
    "Cluster 6 Characteristics:\n",
    "- Highest scoring group (PTS)\n",
    "- Highest assists and turnovers (AST, TOV, AST%, AST/TO, AST_RATIO)\n",
    "- Highest usage (USG%, POSS)\n",
    "- Smallest player group\n",
    "- Best shot creation (unassisted FGM, PULLUP_PTS, PULLUP_FG%)\n",
    "- Best hustle stats (CHARGES_DRAWN, LOOSE_RECOVERED_TOTAL, STL)\n",
    "- Best 3-level scoring (DRIVE_PTS, CATCHSHOOT_FG%, PULLUP_PTS, %PTS_MidRange)\n",
    "\n",
    "Cluster 6: Floor Generals\n",
    "\n",
    "Notable players\n",
    "- Luka Doncic (2020-2021)\n",
    "- Trae Young (2020-2022)\n",
    "- Ja Morant (2020-2022)\n",
    "- Donovan Mitchell (2021-2022)\n",
    "- James Harden (2020-2022)\n",
    "- Chris Paul (2020-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2693032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans7.to_csv('C:/Users/imdan/downloads/data/df_kmeans7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e40023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 0, 'kmeans_label'] = 'Offensive-Minded Guards'\n",
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 1, 'kmeans_label'] = 'Mobile Bigs'\n",
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 2, 'kmeans_label'] = 'Offensive-Minded Perimeter Wings'\n",
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 3, 'kmeans_label'] = 'Versatile Power Forwards'\n",
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 4, 'kmeans_label'] = 'Stationary Interior Bigs'\n",
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 5, 'kmeans_label'] = '3 and D Wings'\n",
    "df_kmeans7.loc[df_kmeans7['kmeans_label'] == 6, 'kmeans_label'] = 'Floor Generals'\n",
    "\n",
    "df_kmeans7.to_csv('C:/Users/imdan/downloads/data/df_kmeans7_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76863e13",
   "metadata": {},
   "source": [
    "One thing to note is that there were a few other models (GMM and Agglomerative) that were experimented with but the silhouette scores did not improve, so they were not examined more deeply -> you can check notebook number 3 to see the results of those other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c976883",
   "metadata": {},
   "source": [
    "# Recommender\n",
    "\n",
    "The next thing that I want to do is create a simple recommender system using a KNN model. I will use the KNeighborsClassifier estimator using the labels from the clustering process and optimize for accuracy score. From there, I can create a recommender system based on the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bf966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of X for KNN\n",
    "\n",
    "X_knn = X.copy()\n",
    "X_knn['CLUSTER'] = kmeans_labels\n",
    "X_knn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn.loc[X_knn['CLUSTER'] == 0, 'CLUSTER'] = 'CLUSTER 0'\n",
    "X_knn.loc[X_knn['CLUSTER'] == 1, 'CLUSTER'] = 'CLUSTER 1'\n",
    "X_knn.loc[X_knn['CLUSTER'] == 2, 'CLUSTER'] = 'CLUSTER 2'\n",
    "X_knn.loc[X_knn['CLUSTER'] == 3, 'CLUSTER'] = 'CLUSTER 3'\n",
    "X_knn.loc[X_knn['CLUSTER'] == 4, 'CLUSTER'] = 'CLUSTER 4'\n",
    "X_knn.loc[X_knn['CLUSTER'] == 5, 'CLUSTER'] = 'CLUSTER 5'\n",
    "X_knn.loc[X_knn['CLUSTER'] == 6, 'CLUSTER'] = 'CLUSTER 6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836cdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = X_knn.drop('CLUSTER', axis=1)\n",
    "y = X_knn['CLUSTER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9479b8",
   "metadata": {},
   "source": [
    "I will start with a baseline model just to get a sense of the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602759db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mm_scaler.fit_transform(X_train)\n",
    "X_test = mm_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a434846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.75)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_PCA = pca.transform(X_train)\n",
    "X_test_PCA = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "neighbors = range(1, 300, 2)\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for n in neighbors: \n",
    "    #Instantiate and Fit\n",
    "    KNN_model = KNeighborsClassifier(n_neighbors=n, metric='cosine')\n",
    "    KNN_model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    #Score the model\n",
    "    train_accuracy = KNN_model.score(X_train, y_train)\n",
    "    test_accuracy = KNN_model.score(X_test, y_test)\n",
    "    \n",
    "    \n",
    "    #Append my accuracy\n",
    "    train_acc.append(train_accuracy)\n",
    "    test_acc.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fb904",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot the graph\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(neighbors, test_acc, color=\"red\", label=\"test\")\n",
    "plt.plot(neighbors, train_acc, color=\"blue\", label=\"train\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.title(\"KNN Graph\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ba11f",
   "metadata": {},
   "source": [
    "We got an accuracy score close to 65% for our testing set. I will now implement a gridsearch in order to optimize hyperparameters and try to get the accuracy score as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a512b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_knn.drop('CLUSTER', axis=1)\n",
    "y = X_knn['CLUSTER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2826a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38346a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "estimators = [('normalise', MinMaxScaler()),\n",
    "              ('reduce_dim', PCA()),\n",
    "              ('knn', KNeighborsClassifier(n_jobs=-1))]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "#visualize the pipeline\n",
    "set_config(display ='diagram')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "params = {'normalise': [MinMaxScaler(), StandardScaler(), RobustScaler()],\n",
    "          'knn__weights': ['uniform','distance'],\n",
    "          'knn__n_neighbors': [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40], \n",
    "          'knn__algorithm': ['auto', 'brute'],\n",
    "          'knn__metric': ['minkowski', 'cosine'],\n",
    "          'reduce_dim__n_components': [.7, .75, .8, .85, .9]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=12)\n",
    "    \n",
    "fitted_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16985",
   "metadata": {},
   "source": [
    "Accounting for all the significant hyperparameters, the highest score achieved was 0.61 which isn't amazing. Before moving forward, I will try again using LDA as dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7c909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_knn.drop('CLUSTER', axis=1)\n",
    "y = X_knn['CLUSTER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18d7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = [('reduce_dim', LDA(solver='eigen')), \n",
    "              ('knn', KNeighborsClassifier(metric='cosine'))]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "#visualize the pipeline\n",
    "set_config(display ='diagram')\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4775245",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "params = {'knn__weights': ['uniform','distance'], \n",
    "          'knn__algorithm': ['auto', 'brute'],\n",
    "          'knn__metric': ['minkowski', 'cosine'],\n",
    "          'knn__n_neighbors': [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40],\n",
    "          'reduce_dim__shrinkage': [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "X = X_knn.drop('CLUSTER', axis=1)\n",
    "y = df['POSITION']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.33, random_state=12)\n",
    "    \n",
    "fitted_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973bdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5b2ce",
   "metadata": {},
   "source": [
    "With the parameters shown above, I was able to get a score of 0.78 which is significantly better than using PCA again. I will create the recommender system with the parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ef146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "X = X_knn.drop('CLUSTER', axis=1)\n",
    "y = X_knn['CLUSTER']\n",
    "\n",
    "lda = LDA(solver='eigen', shrinkage=0.001)\n",
    "lda_fit = lda.fit(X,y)\n",
    "X_lda = lda_fit.transform(X)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=18)\n",
    "\n",
    "knn_fit = knn.fit(X_lda, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205cb0e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_fit.score(X_lda, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = X.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb751a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_final = pd.DataFrame(data=X_lda, index=X0['index'], columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbef8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_fit.kneighbors(X_final.loc[X_final.index == 'JordanPoole|2022'], n_neighbors=5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit.kneighbors(X_final.loc[X_final.index == 'JordanPoole|2022'], n_neighbors=5)[0][0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_recommend(player, n=5):\n",
    "    players_list = []\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        players_list.append(knn_fit.kneighbors(X_final.loc[X_final.index == player], n_neighbors=n)[1][0][i])\n",
    "        distances.append(knn_fit.kneighbors(X_final.loc[X_final.index == player], n_neighbors=n)[0][0][i])\n",
    "        \n",
    "    for player in players_list:\n",
    "        print(f\"player: {X0.iloc[player][0]}\")\n",
    "    for distance in distances:\n",
    "        print(f\"distances: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e41cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player_recommend('GiannisAntetokounmpo|2022', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902a55d",
   "metadata": {},
   "source": [
    "To compare my algorithm with another, I will create a simple recommender system using cosine similarity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe271288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_index = X_practice[X_practice['index'] == 'GiannisAntetokounmpo|2022'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2254024",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(X, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame({'PLAYER':X_practice['index'], \n",
    "                       'similarity': np.array(similarities[player_index, :]).squeeze()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the top 10 most similar movies\n",
    "sim_df.sort_values(by='similarity', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a634e",
   "metadata": {},
   "source": [
    "With this simple recommender system, we can say that the 2 recommenders output quite different results. There are limitations to both recommender systems. This will be explored further in the analysis report (same github repo)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
